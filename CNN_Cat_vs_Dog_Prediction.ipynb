{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sCV30xyVhFbE",
    "outputId": "1c4efc96-6b1c-49e1-f63b-7a1375c94597"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Pant\\Desktop\\DEEP LEARNING COURSE\\CNN (Image Classification)\\CNN summarised\\Section 40 - Convolutional Neural Networks (CNN)\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Kevin Pant\\Desktop\\DEEP LEARNING COURSE\\CNN (Image Classification)\\CNN summarised\\Section 40 - Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kevin Pant\\\\Desktop\\\\DEEP LEARNING COURSE\\\\CNN (Image Classification)\\\\CNN summarised\\\\Section 40 - Convolutional Neural Networks (CNN)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FIleuCAjoFD8",
    "outputId": "0ad77db7-e30f-413e-e343-997c077a598c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ek2qQYHlcg7a"
   },
   "source": [
    "The first thing we'll do is we will apply some <br/>transformations on all the images of the training set.<br/>\n",
    "\n",
    "The images of the training set only we want apply these same transformations on the test.<br/>\n",
    "\n",
    "The reason why we want to apply some transformations on the images of the training set is for only one purpose.<br/>\n",
    "\n",
    "It is to avoid overfilling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byrTWN41c91D"
   },
   "source": [
    "If we don't apply these transformations when training our CNN on the training set, we</br>\n",
    "will get a huge difference between the accuracy on the training set and the one on the test set.</br>\n",
    "\n",
    "On the evaluation set, actually, we will get very high accuracy on the training set, you know, close to 98 percent and much lower accuracy on the test.</br>\n",
    "\n",
    "And that is called Overfitting.</br>\n",
    "That's something we absolutely need to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yshGLN1od0jq"
   },
   "source": [
    "What are these transformations? <br/>\n",
    "\n",
    "They're some simple geometrical transformations or some zooms or some rotations on your images.<br/>\n",
    "\n",
    "Basically, we're going to apply some:<br/>\n",
    "1.Geometrical transformations like transaction's to shift some ofmthe pixels.<br/>\n",
    "\n",
    "2.Then we're gonna rotate the images, we're gonna do some horizontal flips.<br/>\n",
    "\n",
    "3.We're gonna do some zoom in and zoom out.<br/>\n",
    "\n",
    "we're going to play a series of transformation so as to modify the images and get them, as we say, \"augmented\".<br/>\n",
    "\n",
    "The technical term of what we're gonna do now with all these transformations is called image augmentation, which consists basically of transforming your images of the training set so that your CNN model doesn't overlearned i.e. it is not overtrained on the existing images,<br/>\n",
    "\n",
    "because by applying these transformations, we will get new images, which is the reason. Why we call this image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0l0YNLtMXnpY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,##(this is feature scaling.)\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,##(How many images in each batch)\n",
    "        class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtbhWw06XnLO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smGuV-GwXl0v"
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKdwtwUgjHdM"
   },
   "source": [
    "1. filters =  how many feature detectors we want?<br/>\n",
    "Classic one is 32 filters in first conv layer\n",
    "2. Kernal = RGB = 3<br/>\n",
    "3. Activation function<br/>\n",
    "4. Input shape = 64*63*3(RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY9EgL6HXlQ1"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32,kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling (adding a pooling layer to Conv layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCcTFxrJkY2a"
   },
   "source": [
    "1.Pool_size = size of FRAME (sqr frame)<br/>\n",
    "2.Shiftubg by slide of \"2\", we're sliding by 2 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3_iNCBuXkNN"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkda_0g5XjkE"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32,kernel_size=3, activation ='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening<br/>\n",
    "Flattening the result of all these convolutions and poolings into a one dimensional vector,<br>which will become the input of a future fully connected new network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsQuLshgXiRX"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection<br/>\n",
    "1.Units = number of Neurons = 128 <br/>\n",
    "2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Sg5ppbgXhIq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer<br/>\n",
    "For the activation function, of the output layer, it is NOT recommended to have a \"rectifier activation function\"('relu'), but rather a sigmoid activation function.<br/>\n",
    "That's because, we're doing binary classification.<br/>\n",
    "\n",
    "3.Otherwise, if we were doing Multiclass Classification, we would have used a softmax activationfunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVx1sZ2eXgfq"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN <br/>\n",
    "We're going to connect it to:<br/> \n",
    "1.an optimizer <br/>\n",
    "2.a lost function and<br/>\n",
    "3.some metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty8uhtZEoKA4"
   },
   "source": [
    "Now we're going to choose once again, an **adam optimizer*** \n",
    "to perform ***stochastic gradient descent***,<br/> update the weights in order to reduce the loss error between the predictions and the target.<br/>\n",
    "\n",
    "Then we're going to choose the binary cross entropy loss once again, because we're doing a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3nA978PXc4L"
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zImRrIz4pQIo"
   },
   "source": [
    "We're not only training the CNN on the train set, but also evaluating it at the same time on the test set.<br/>\n",
    "\n",
    "And that second parameter corresponds exactly to this.<br/>\n",
    "\n",
    "We have to specify here the validation data.\n",
    "\n",
    "That's the name of the parameter.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-KNOuA9pjv5"
   },
   "source": [
    "But that is of course, the set on which we want to evaluate our CNN that is our ***test set*** that will be the value of the parameter.<br/>\n",
    "\n",
    "But the name of that parameter is, as I just said, <br/>validation_data = test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LDXAtG8qOdB"
   },
   "source": [
    "***EPOCH MEANING***<br/>\n",
    "Epoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYmvqoq0XcRQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 207s 829ms/step - loss: 0.6623 - accuracy: 0.5993 - val_loss: 0.5910 - val_accuracy: 0.7025\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 94s 377ms/step - loss: 0.5931 - accuracy: 0.6831 - val_loss: 0.6280 - val_accuracy: 0.6705\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 123s 491ms/step - loss: 0.5539 - accuracy: 0.7095 - val_loss: 0.5692 - val_accuracy: 0.7150\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 117s 469ms/step - loss: 0.5218 - accuracy: 0.7359 - val_loss: 0.6392 - val_accuracy: 0.6475\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 91s 365ms/step - loss: 0.4955 - accuracy: 0.7613 - val_loss: 0.4915 - val_accuracy: 0.7620\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.4762 - accuracy: 0.7710 - val_loss: 0.4559 - val_accuracy: 0.7935\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 87s 347ms/step - loss: 0.4586 - accuracy: 0.7825 - val_loss: 0.4635 - val_accuracy: 0.7855\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 84s 338ms/step - loss: 0.4486 - accuracy: 0.7885 - val_loss: 0.4558 - val_accuracy: 0.7905\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 85s 342ms/step - loss: 0.4290 - accuracy: 0.7960 - val_loss: 0.5147 - val_accuracy: 0.7605\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 84s 338ms/step - loss: 0.4338 - accuracy: 0.7996 - val_loss: 0.4710 - val_accuracy: 0.7815\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 0.4097 - accuracy: 0.8117 - val_loss: 0.4419 - val_accuracy: 0.8095\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 87s 348ms/step - loss: 0.3993 - accuracy: 0.8169 - val_loss: 0.5032 - val_accuracy: 0.7595\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 93s 370ms/step - loss: 0.3899 - accuracy: 0.8177 - val_loss: 0.4511 - val_accuracy: 0.7990\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 89s 357ms/step - loss: 0.3793 - accuracy: 0.8279 - val_loss: 0.4501 - val_accuracy: 0.8070\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 93s 371ms/step - loss: 0.3730 - accuracy: 0.8315 - val_loss: 0.4594 - val_accuracy: 0.8040\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 93s 372ms/step - loss: 0.3574 - accuracy: 0.8390 - val_loss: 0.4739 - val_accuracy: 0.7910\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 94s 376ms/step - loss: 0.3514 - accuracy: 0.8390 - val_loss: 0.4920 - val_accuracy: 0.7965\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 92s 368ms/step - loss: 0.3315 - accuracy: 0.8554 - val_loss: 0.4553 - val_accuracy: 0.8125\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 94s 375ms/step - loss: 0.3306 - accuracy: 0.8519 - val_loss: 0.4609 - val_accuracy: 0.8065\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 90s 360ms/step - loss: 0.3124 - accuracy: 0.8639 - val_loss: 0.4552 - val_accuracy: 0.8125\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 91s 364ms/step - loss: 0.3083 - accuracy: 0.8655 - val_loss: 0.4710 - val_accuracy: 0.8075\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 90s 360ms/step - loss: 0.2917 - accuracy: 0.8776 - val_loss: 0.5098 - val_accuracy: 0.7915\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 108s 433ms/step - loss: 0.2842 - accuracy: 0.8825 - val_loss: 0.4740 - val_accuracy: 0.8080\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 106s 424ms/step - loss: 0.2702 - accuracy: 0.8861 - val_loss: 0.5444 - val_accuracy: 0.7940\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 96s 386ms/step - loss: 0.2615 - accuracy: 0.8871 - val_loss: 0.5142 - val_accuracy: 0.8105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c6e8fc748>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\test\\lib\\site-packages\\PIL\\Image.py\n"
     ]
    }
   ],
   "source": [
    "## ONLY IF ERROR OCCURS SAYING \"cannot read image.XYZ \"\n",
    "from PIL import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction <br/>\n",
    "deploying our model on the two images of this single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaNTceEFXbf_"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-34-6bad1b34aacd>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-6bad1b34aacd>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    test_image = image.load_img('C:\\Users\\Kevin Pant\\Desktop\\DEEP LEARNING COURSE\\CNN (Image Classification)\\CNN summarised\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\single_prediction/_cat_or_dog_1.jpg', target_size=(64,64))\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "##the next step will be, of course, to load that single image on which we want to deploy our\n",
    "##moral to predict if there is a cat or dog inside\n",
    "\n",
    "## I'm going to create here a new variable that will be exactly that image\n",
    "\n",
    "##Second param v. imp:\n",
    "## image has to have the same size as the one that was used during the training.\n",
    "## We actually resized our images into size of 64 by 64.\n",
    "\n",
    "test_image = image.load_img('C:\\Users\\Kevin Pant\\Desktop\\DEEP LEARNING COURSE\\CNN (Image Classification)\\CNN summarised\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\single_prediction/_cat_or_dog_1.jpg', target_size=(64,64))\n",
    "\n",
    "##convert this test image into an array.\n",
    "\n",
    "## the predict method has to be called on the exact same format that was used during the training.\n",
    "\n",
    "##tf.keras.preprocessing.image.img_to_array(img, data_format=None, dtype=None)\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# I'm gonna call this function, which allows exactly to add a fake dimension corresponding to \"BATCH SIZE\" = np.expand\n",
    "## axis = 0 -the dimension of the batch, which we're adding to our image will be the first dimension.\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "##Figure whether 1 = Cat or Dog\n",
    "\n",
    "training_set.class_indicies\n",
    "if result [0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error occurs due to the name of my directory.\n",
    "##### I sugest to give a simple name and path for your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M88CuAaqvnF3"
   },
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLQyHgaCXbck"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of convolutional_neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
